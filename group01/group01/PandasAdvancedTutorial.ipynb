{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Pandas Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets generate some random data for a dataframe\n",
    "d = {}\n",
    "d['index'] = list(range(100))\n",
    "d['colA'] = [random.randint(1, 100) for i in range(100)]\n",
    "d['colB'] = [random.randint(1, 100) for i in range(100)]\n",
    "d['colC'] = [random.randint(1, 100) for i in range(100)]\n",
    "df = pd.DataFrame(data=d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('index')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create a new column in our dataframe by simply assigning a static value\n",
    "df['colD'] = 0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but assigning a static value is not very useful\n",
    "# lets create a function that we can apply to each row\n",
    "\n",
    "def foo(val):\n",
    "    \"\"\"\n",
    "    @param val - integer value\n",
    "    @return - True if val < 50, False otherwise\n",
    "    \"\"\"\n",
    "    return True if val < 50 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets apply foo to column D (with column C as input) with a lambda function\n",
    "df['colD'] = df.apply(lambda x: foo(x['colC']), axis=1)   # x represents a row, which can be indexed by column name\n",
    "                                                          # axis=1 is required to traverse the dataframe by rows\n",
    "                                                          #     (by default, axis=0 will traverse df by columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "# and we notice colD contains True/False based on the value of colC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent function calls\n",
    "df['colD'] = df['colC'].apply(lambda x: foo(x))  # apply to the colC column/series, instead of the dataframe\n",
    "                                                 # useful if you are not referencing more than one column in the application\n",
    "# or\n",
    "df['colD'] = df.apply(lambda x: True if x['colC'] < 50 else False, axis=1) # inline rewrite of foo()\n",
    "# or\n",
    "df['colD'] = df['colC'].apply(lambda x: True if x < 50 else False) # merging the above two approaches\n",
    "\n",
    "df.head()\n",
    "# still the same result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But why would I need to create new columns?\n",
    "\n",
    "This is incredibly useful for annotating data.  \n",
    "For example, in MP1, you can use df.apply() to:\n",
    " * Store the number of bit flips based on the syndrome column\n",
    " * Mark coalesced enries with True/False; You can filter out coalesced entries based on this new column\n",
    " * Identify error reasons & suberror reasons by splitting the 'Error Type' column into multiple new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing column values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can update column values in df with =\n",
    "\n",
    "# First, index into the row you want to modify with iloc\n",
    "df.iloc[0]['colA'] = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARGH, pandas won't allow us to modify the dataframe\n",
    "Lets do this the right way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.at[0, 'colA'] = 999\n",
    "# or df.loc[0, 'colA'] = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "# and 'colA' at index 0 has been updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets try slixing the dataframe and then modifying it\n",
    "df_slice = df[df['colA'] < 10] # select entries with colA < 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are left with 9 entries\n",
    "# lets modify df_slice at index 12\n",
    "df_slice.at[12, 'colA'] = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, that was easy\n",
    "# But has our original df been modified?\n",
    "df.at[12, 'colA']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nope. Keep this in mind when modifying slices. The original dataframe will not be modified.\n",
    "# To modify the original dataframe, you must operate on it directly\n",
    "# (This will come in handy when coalescing your dataset, you will have to operate on slides of unique nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizing your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets say you want to call a certain function on a set of different inputs\n",
    "# (Once again, this will be useful in coalescing)\n",
    "\n",
    "\n",
    "# generate a set of dummy inputs\n",
    "inputs = [random.randint(1, 100) for i in range(100)]\n",
    "print(inputs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, now lets define a function that we will apply to our set of inputs\n",
    "def square(i):\n",
    "    \"\"\"\n",
    "    @param i - input\n",
    "    @return - square of i\n",
    "    \"\"\"\n",
    "    return i**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could just use a for loop here\n",
    "for i in inputs:\n",
    "    print (square(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# or we could do this in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "# create a pool with 8 threads (increase or reduce this based on the number of threads your processor supports)\n",
    "pool = ThreadPool(8) \n",
    "poolresults = pool.map(square, inputs) # map the square function to inputs\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# poolresults contains our returned values\n",
    "poolresults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We strongly recommend using the ThreadPool approach to coalesce your data.\n",
    "\n",
    "* Create a list of dataframe slices, each slice containing entries for a unique node.\n",
    "\n",
    "* Next, define your Sliding Window Algorithm function such that it returns the number of tuples after coalescing its slice. You should also mark any rows that you will eventually filter out.\n",
    "\n",
    "* Then, create a ThreadPool and map your algorithm function to the list of dataframe slices\n",
    "\n",
    "* Run the pool with close/join. The return value of the threadpool map will give you your tuple counts. You just need to sum up this returned list to get the datapoint for your knee curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
